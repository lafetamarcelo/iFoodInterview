{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier\n",
    "\n",
    "\n",
    "In this section we will use the soo called XGBoost library to build a classifier, to use the costumer information to predict the probable costumer to comply in the next marketing campaing. This algorithm was chosen, considering its high performance on both computational and accuracy manners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_path = './ifood-data-business-analyst-test/ml_project1_data.csv'\n",
    "dataset = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "\n",
    "Here we need to provide a simple preprocess to the data to remove possible non informative data, to create information fields that are more suitable for interpretation, some encoding of the features (since some are categorical), also we will make some normalization on the data to avoid over weighting errors and so on... \n",
    "\n",
    "> Notice that most functions to do the preprocessing here are implemented in a separated code, since it could be used for other models, and for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "The preprocessing pipeline, for the XGBoost classification algorithm will be the one, as follows:\n",
    "\n",
    "- `Step #1` First we will replace some fields with more interpretable information (Birth date => Age, Customer Registration => Persistence, ...)\n",
    "\n",
    "- `Step #2` Then we are going to replace the categorical data set with an encoded one (categorical variables => numerical variables)\n",
    "\n",
    "- `Step #3` Then some non informative features will be dropped from the analysis, _e.g._ features that are constant in all samples (which does not provide any information)\n",
    "\n",
    "- `Step #4` Since we have only 24 samples with NaN (or null) values, we can drop those from the dataset, instead of concerning with interpolation and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features dropped: ['Z_CostContact', 'Z_Revenue']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = support.replaceFields(dataset)             # Step #1\n",
    "\n",
    "dataset, encoders = support.encodeDataSet(dataset)   # Step #2\n",
    "\n",
    "dataset = support.dropNonInformative(dataset)        # Step #3\n",
    "\n",
    "df = dataset.dropna()                                # Step #4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some particular processing of the data for this particular XGBoost classifier algorithm. Notice that the dataset is not balanced between `1` and `0` on the output... Actually it is a proportion close to 10% of `1`/`0`. Therefore, something must be done to deal with the unbalanced dataset. Here we are not doing a simple error weighting using the output data proportions... We will use a randomized sample technique, the reason is:\n",
    "\n",
    "> _Without knowing depply the fenomenom, a simple approach of just weighting the error proportionally can be very dangerous, even though it is more straightforward. The reason behind this is related to the fact that you are only weighting the data based on the variance of the output feature, and it is not considering how that weighting will behave on the other features variances. If this breaf explanation did not trigger something that made you undestand the idea, please check out my book at [IET Digital Library](https://digital-library.theiet.org/content/books/10.1049/pbce123e_ch3;jsessionid=ji7b4180pudn.x-iet-live-01) where I explain in details every math behind this resolution._\n",
    "\n",
    "In summary, we will first build the regression problem (yes!! For mathematicians the classification is a binary regression problem... It is common sense to say that regression, is when one wants to fit a curve to the data, that is actually a linear regression) as something close to the structure:\n",
    "\n",
    "$$y(k) = f(\\phi(k), \\theta)$$\n",
    "\n",
    "After we can normalize the data, by just making it fit the the group $[0, 1]$. Notice that we do not want to normalize the variance of each feature... Since the eventual birth of the deep searching algorithms, where most of them use stochastic searching algorithms (simillar to the old annealing, brunch and bound ...), the noramlization of the variance usually removes some scotachastic properties (in a commom sense, removes some part of the randomicity pattern) of the feature. \n",
    "\n",
    "Therefore, after normalizing we will balance the data accordenlly with the description provided before by randomly selecting variables to make shure that we have 50% `1`s and 50% `0`s. Of course this will reduce the dataset, but one will see that with stochastic search algorithms, such as the one used by XGBoost, there will not compromise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Creating the regression format\n",
    "phi = df.loc[:, ~df.columns.isin(['Response', 'ID'])].to_numpy()\n",
    "target = df[\"Response\"].to_numpy()\n",
    "\n",
    "# Normalization\n",
    "max_vals = np.amax(phi, axis=0)\n",
    "min_vals = np.amin(phi, axis=0)\n",
    "phi_n = (phi - max_vals) / (max_vals - min_vals)\n",
    "\n",
    "# Balancing the data\n",
    "X, y = support.balanceDataSet(phi_n, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train data\n",
    "\n",
    "Here we have a simple segregation of the preprocessed dataset into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters\n",
    "\n",
    "Here we will tune the model hyper parameters using a particular algorithm that I usually enjoy, the annealing search. This algorithm actually uses a stochastic search (random search) based on the information entropy of the data, and is actually a global optimization algorithm. This means that it does not use derivatives to search for the optimum set of hyper parameters, it actually is something of a grid search where at each iteration the next set of hyper parameters are not defined by a grid relation, but actually by its randomicity probability of encreasing the information (entropy) of the error of the function that it wants to minimize. And believe it or not, it always finds the optimum set inside the provided restrictions. It is pretty cool!!\n",
    "\n",
    "So to use it, we must define some bounderies for the parameters that we want to search, a cost function, that here it is the sum of the false positives (since we want to minimize this in the predictions). Then we just need to pass to the algorithm and wait to search for the best parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.28713858424725824\n",
       " message: ['Maximum number of iteration reached']\n",
       "    nfev: 12778\n",
       "    nhev: 0\n",
       "     nit: 2000\n",
       "    njev: 0\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([6.85912944, 2.27341813, 2.33959873])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating the parameters bounderies\n",
    "lower, upper = [1, 1, 0.01], [25, 25, 15]\n",
    "bounds = zip(lower, upper)\n",
    "\n",
    "# Run the annealing searching \n",
    "pars, res = support.xgbHyperGridSearch(bounds, (y_train, X_train, y_test, X_test))\n",
    "res # print the annealing search summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "Then we can use the parameters found to create the best XGBoost model classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=2.3395987276390073,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints=None,\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=2, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n",
       "              validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create the model\n",
    "model = xgb.XGBClassifier(max_depth=pars[0], min_child_weight=pars[1], gamma=pars[2])\n",
    "# Train the model\n",
    "model.fit(X_train, y_train,\n",
    "          eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "          eval_metric='logloss', \n",
    "          verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'get_fscore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-511d40f8eed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mproper_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Response'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'get_fscore'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the correct label for the features\n",
    "proper_scores = dict()\n",
    "names = df.loc[:, ~df.columns.isin(['Response', 'ID'])].keys()\n",
    "scores = model.get_fscore()\n",
    "for score in scores:\n",
    "    if len(score) == 2:\n",
    "        index = int(score[-1])\n",
    "    else:\n",
    "        index = int(score[-2:])\n",
    "    proper_scores[names[index]] = scores[score]\n",
    "\n",
    "xgb.plot_importance(proper_scores, grid=False, height=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "Then here we can build some visualizations to show the results. The first one is the fitting with the testing data, using the confusion matrix to make sure the model is consistent. \n",
    "\n",
    "> Remember that this data set is balanced, and therefore is a more honnest result, then the second one that will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "index = ['False','True']  \n",
    "cm_df = pd.DataFrame(conf_mat,index,index)                      \n",
    "\n",
    "plt.figure(figsize=(10,6))  \n",
    "sns.heatmap(cm_df, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can show the classification performance of the unbalaced dataset, using all samples, to be sure that the model maintain its consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(phi_n)\n",
    "conf_mat = confusion_matrix(target, y_pred, normalize='true')\n",
    "\n",
    "index = ['False','True']  \n",
    "cm_df = pd.DataFrame(conf_mat,index,index)                      \n",
    "\n",
    "plt.figure(figsize=(10,6))  \n",
    "sns.heatmap(cm_df, annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Lets save the model for further use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "pickle_data = {\n",
    "    \"model\" : model,\n",
    "    \"data\" : (phi_n, target)\n",
    "}\n",
    "\n",
    "with open('XGBoost.pickle', 'wb') as f:\n",
    "    pickle.dump(pickle_data, f)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
